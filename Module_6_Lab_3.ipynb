{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0hAW8ptqVeyP"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zoeera/FMML-projects-and-labs/blob/main/Module_6_Lab_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Probabilistic ML models\n",
        "\n"
      ],
      "metadata": {
        "id": "V89R735GVNdJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Convolutional Operations"
      ],
      "metadata": {
        "id": "0hAW8ptqVeyP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igR7HFGhRRdm"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2D 3x3 binary image with vertical edge\n",
        "image1 = np.array([[1,1,0],\n",
        "                   [1,1,0],\n",
        "                   [1,1,0]])\n",
        "\n",
        "# 2D 3x3 binary image with horizontal edge\n",
        "image2 = np.array([[0,0,0],\n",
        "                   [0,0,0],\n",
        "                   [1,1,1]])\n",
        "\n",
        "# print(image1*255)\n",
        "# Let's plot the images\n",
        "fig = plt.figure(figsize=(10,4))\n",
        "ax = fig.add_subplot(1,2,1)\n",
        "ax.imshow(image1, cmap='gray', extent=[0, 3, 3, 0])\n",
        "# plt.ylim(0, 3)\n",
        "ax.set_title('Image 1 with vertical edge')\n",
        "\n",
        "ax = fig.add_subplot(1,2,2)\n",
        "ax.imshow(image2, cmap='gray', extent=[0, 3, 3, 0])\n",
        "ax.set_title('Image 2 with horizontal edge')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1WlakMr1Wlee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2ralw9w7R0x"
      },
      "source": [
        "# Vertical Line filter\n",
        "filter = np.array([[1,0,-1],\n",
        "                   [1,0,-1],\n",
        "                   [1,0,-1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoWLcsu37jLq"
      },
      "source": [
        "# Applying filter to first image\n",
        "output = np.sum(np.multiply(image1, filter))\n",
        "print('Output from first image: ', output)\n",
        "\n",
        "# Applying filter to second image\n",
        "output = np.sum(np.multiply(image2, filter))\n",
        "print('Output from second image: ', output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl7h6HrJ8h5W"
      },
      "source": [
        "# Horizontal edge filter\n",
        "filter = np.array([[-1,-1,-1],\n",
        "                   [ 0, 0, 0],\n",
        "                   [ 1, 1, 1]])\n",
        "\n",
        "output = np.sum(np.multiply(image1, filter))\n",
        "print('Output from first image: ', output)\n",
        "\n",
        "output = np.sum(np.multiply(image2, filter))\n",
        "print('Output from second image: ', output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VszqKiZHsOXB"
      },
      "source": [
        "def apply_filter(img, filter):\n",
        "  height, width = img.shape\n",
        "  filter_size = filter.shape\n",
        "\n",
        "  output = np.empty(0)\n",
        "\n",
        "  # Move the filter over entire image and store the result in output\n",
        "  for i in range(0, height - filter_size[1] + 1):\n",
        "    for j in range(0, width - filter_size[0] + 1):\n",
        "      # Matrix multiplication for a single patch of image and filter\n",
        "      output = np.append(output, np.sum(np.multiply(img[i:i+filter_size[0], j:j+filter_size[1]], filter)))\n",
        "\n",
        "  # Calculate the output shape of the resultant image\n",
        "  output_shape = (height - (filter_size[1]-1)), (width - (filter_size[0]-1))\n",
        "\n",
        "  # Return the reshaped image\n",
        "  return output.reshape(output_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Y03Dtn3F0sp"
      },
      "source": [
        "Plotting function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5mJqbXQZF0sq"
      },
      "source": [
        "def plot_images(images, titles, tick_params=True):\n",
        "  n = len(images)\n",
        "  fig = plt.figure(figsize=(10,4))\n",
        "  for i in range(n):\n",
        "    ax = fig.add_subplot(1,n,i+1)\n",
        "    if len(images[i].shape) == 2:\n",
        "      ax.imshow(images[i], cmap='gray',\n",
        "                extent=(0,images[i].shape[1], images[i].shape[0], 0))\n",
        "    else:\n",
        "      ax.imshow(images[i])\n",
        "    ax.set_title(titles[i])\n",
        "    if not tick_params:\n",
        "      plt.tick_params(axis='both', labelbottom=False, bottom=False,\n",
        "                labelleft=False, left=False)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vpTLmPYG0Pom"
      },
      "source": [
        "# 2D image\n",
        "img = np.array([[20,20,0,0,0],\n",
        "                [20,20,0,0,0],\n",
        "                [20,20,0,0,0],\n",
        "                [20,20,0,0,0],\n",
        "                [20,20,0,0,0]])\n",
        "\n",
        "# Vertical edge filter\n",
        "filter = np.array([[1,0,-1],\n",
        "                   [1,0,-1],\n",
        "                   [1,0,-1]])\n",
        "\n",
        "\n",
        "output = apply_filter(img, filter)\n",
        "print(output) # Note the shape of output image!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-tmlp1qHA9f"
      },
      "source": [
        "# Let's plot the above image with results\n",
        "images = []\n",
        "titles = []\n",
        "\n",
        "images.append(img)\n",
        "titles.append('Original Image')\n",
        "\n",
        "images.append(filter)\n",
        "titles.append('Filter')\n",
        "\n",
        "images.append(output)\n",
        "titles.append('Convolution Output')\n",
        "\n",
        "plot_images(images, titles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmSCGzaYVFze"
      },
      "source": [
        "As, you can see, horizontal edge is detected in the output.\n",
        "\n",
        "Now, we will see the effect of applying this filter on a grayscale image. Again, for this, we need to 'convolve' the filter over the entire image.\n",
        "We will use the same filter and function defined earlier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjOQM6tdF0sr"
      },
      "source": [
        "# Get the sample image\n",
        "!curl -L -o 'lotus.jpg' 'https://drive.google.com/uc?export=download&id=1gQSQlrUws22KLRUacXwvN1G8FtIyhfGt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04P_GfojIo4u"
      },
      "source": [
        "# Read the image with opencv, 0 stands for 'grayscale'\n",
        "image = cv2.imread('lotus.jpg', 0)\n",
        "print('Original image size: ', image.shape)\n",
        "\n",
        "# Saving images for plots\n",
        "images = []\n",
        "titles = []\n",
        "\n",
        "images.append(image)\n",
        "titles.append('Original Image')\n",
        "\n",
        "# Vertical edge filter\n",
        "filter = np.array([[1,0,-1],\n",
        "                   [1,0,-1],\n",
        "                   [1,0,-1]])\n",
        "\n",
        "images.append(filter)\n",
        "titles.append('Filter')\n",
        "\n",
        "# Apply this filter to image\n",
        "output = apply_filter(image, filter)\n",
        "\n",
        "print('Output image size: ', output.shape)\n",
        "\n",
        "images.append(output)\n",
        "titles.append('Convolution Output')\n",
        "\n",
        "# Let's plot the images\n",
        "plot_images(images, titles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise\n",
        "1. Try padding in convolution on lotus.jpg and show results\n"
      ],
      "metadata": {
        "id": "bsQxPFm-YDmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Load image\n",
        "image = cv2.imread('lotus.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# Add padding\n",
        "padded_image = cv2.copyMakeBorder(image, 1, 1, 1, 1, cv2.BORDER_CONSTANT, value=0)\n",
        "\n",
        "# Define and apply kernel\n",
        "kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
        "convolved_image = cv2.filter2D(padded_image, -1, kernel)\n",
        "\n",
        "# Display results\n",
        "plt.subplot(131), plt.imshow(image, cmap='gray'), plt.title('Original')\n",
        "plt.subplot(132), plt.imshow(padded_image, cmap='gray'), plt.title('Padded')\n",
        "plt.subplot(133), plt.imshow(convolved_image, cmap='gray'), plt.title('Convolved')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9F7-LIHU9tLV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Try stride in convolution on lotus.jpg and show results"
      ],
      "metadata": {
        "id": "_yGjsEsZ-XvO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Load image\n",
        "image = cv2.imread('lotus.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# Define kernel and stride\n",
        "kernel = np.array([[0, -1, 0], [-1, 5, -1], [0, -1, 0]])\n",
        "stride = 2\n",
        "\n",
        "# Apply convolution with stride\n",
        "output_shape = ((image.shape[0] - kernel.shape[0]) // stride + 1,\n",
        "                (image.shape[1] - kernel.shape[1]) // stride + 1)\n",
        "convolved_image = np.zeros(output_shape)\n",
        "\n",
        "for i in range(0, output_shape[0]):\n",
        "    for j in range(0, output_shape[1]):\n",
        "        region = image[i*stride:i*stride+kernel.shape[0], j*stride:j*stride+kernel.shape[1]]\n",
        "        convolved_image[i, j] = np.sum(region * kernel)\n",
        "\n",
        "# Display results\n",
        "plt.subplot(121), plt.imshow(image, cmap='gray'), plt.title('Original')\n",
        "plt.subplot(122), plt.imshow(convolved_image, cmap='gray'), plt.title('Convolved with Stride')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Qa2VjD_q-Ykr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. CNN and Using Learnt Representations\n",
        "\n",
        "Now lets implement a CNN in pytorch and use the learnt representations for image classification of MNIST dataset."
      ],
      "metadata": {
        "id": "u1hVe_w-aOKe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewhnwh1P6aAk"
      },
      "source": [
        "<img src='https://miro.medium.com/max/1872/1*SGPGG7oeSvVlV5sOSQ2iZw.png' />\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWJa9NQvSdr8"
      },
      "source": [
        "# Import packages\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhYuybK767Sa"
      },
      "source": [
        "# Device configuration (whether to run on GPU or CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZMFwB08ZvSN"
      },
      "source": [
        "# Set seeds for reproducibility\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ0n6wa47uZu"
      },
      "source": [
        "#### Load MNIST data\n",
        "We will use the [MNIST dataset](https://pytorch.org/vision/stable/datasets.html#mnist) from torchvision Pytorch and setup the train and test dataloaders."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nr2uSnxn7nIb"
      },
      "source": [
        "batch_size_train = 128\n",
        "batch_size_test = 128\n",
        "\n",
        "# Images in torchvision datasets are PIL Images in range [0,1] so we need\n",
        "# 'ToTensor' transform to convert them into tensors\n",
        "train_data = torchvision.datasets.MNIST('./data', train=True, download=True,\n",
        "                             transform=torchvision.transforms.ToTensor())\n",
        "test_data = torchvision.datasets.MNIST('./data', train=False, download=True,\n",
        "                             transform=torchvision.transforms.ToTensor())\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size_train, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size_test, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HRj26o673We"
      },
      "source": [
        "#### Understand the dataset\n",
        "Let us now visualize the dataset in terms of number of samples, classes etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwP6Weyr7wZb"
      },
      "source": [
        "print('Training data shape : ', train_data.data.shape, train_data.targets.shape)\n",
        "print('Testing data shape : ', test_data.data.shape, test_data.targets.shape)\n",
        "\n",
        "# Find the unique numbers from the train labels\n",
        "classes = np.unique(train_data.targets.numpy())\n",
        "nClasses = len(classes)\n",
        "print('Total number of outputs : ', nClasses)\n",
        "print('Output classes : ', classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43ZzTl-078NJ"
      },
      "source": [
        "# Helper function to plot data\n",
        "def plot_data(images, labels, classes=None):\n",
        "  figure = plt.figure(figsize=(9, 4))\n",
        "  cols, rows = 5, 2\n",
        "  for i in range(1, cols * rows + 1):\n",
        "      sample_idx = torch.randint(len(images), size=(1,)).item()\n",
        "      img, label = images[sample_idx], labels[sample_idx]\n",
        "      figure.add_subplot(rows, cols, i)\n",
        "      if classes is not None:\n",
        "        label = classes[label]\n",
        "      plt.title('Label:' +str(label))\n",
        "      plt.axis(\"off\")\n",
        "      plt.imshow(img.squeeze(), cmap=\"gray\")\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1bnP8EyqktD"
      },
      "source": [
        "plot_data(train_data.data, train_data.targets.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYYNFnPk8LWD"
      },
      "source": [
        "#### Define the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-e1DrVT8GYt"
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "        self.max_pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1, 1)\n",
        "        self.max_pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        # fully connected layer\n",
        "        self.fc = nn.Linear(64 * 7 * 7, 128)\n",
        "        # output layer 10 classes\n",
        "        self.out = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x) #activation\n",
        "        x = self.max_pool1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.max_pool2(x)\n",
        "        # flatten the output for FC layer\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        output = self.out(x)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqoGCFWd8RRG"
      },
      "source": [
        "# Build the model object and put on the device\n",
        "model = CNN().to(device)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux_2PGgT8diG"
      },
      "source": [
        "#### Define Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZDAwCNb8VER"
      },
      "source": [
        "# Cross Entropy loss for multi-class classification\n",
        "loss_func = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fwSTPmI8lQP"
      },
      "source": [
        "#### Define optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GieENlOa8heQ"
      },
      "source": [
        "# Basic SGD optimizer with 0.01 learning rate\n",
        "optimizer = optim.SGD(model.parameters(), lr = 0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svJ3-UB187qa"
      },
      "source": [
        "#### Train the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t94F0wxMefKz"
      },
      "source": [
        "Helper function for training/testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDXNReyi8n5o"
      },
      "source": [
        "def train(num_epochs, model, train_loader, loss_func, optimizer):\n",
        "\n",
        "  # Training mode\n",
        "  model.train()\n",
        "\n",
        "  train_losses = []\n",
        "  train_acc = []\n",
        "\n",
        "  # Train the model\n",
        "  for epoch in range(num_epochs):\n",
        "    running_loss = 0\n",
        "    running_acc = 0\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "\n",
        "      # clear gradients for this training step\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Put data on devices\n",
        "      images = images.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # Forward pass\n",
        "      output = model(images)\n",
        "\n",
        "      # Calculate loss\n",
        "      loss = loss_func(output, labels)\n",
        "\n",
        "      # Backpropagation, compute gradients\n",
        "      loss.backward()\n",
        "\n",
        "      # Apply gradients\n",
        "      optimizer.step()\n",
        "\n",
        "      # Running loss\n",
        "      running_loss += loss.item()\n",
        "\n",
        "      # indices of max probabilities\n",
        "      _, preds = torch.max(output, dim=1)\n",
        "\n",
        "      # Calculate number of correct predictions\n",
        "      correct = (preds.float() == labels).sum()\n",
        "      running_acc += correct\n",
        "\n",
        "      epoch_loss = running_loss / len(train_loader.dataset)\n",
        "      epoch_acc = running_acc / len(train_loader.dataset)\n",
        "\n",
        "    train_losses.append(epoch_loss)\n",
        "    train_acc.append(epoch_acc)\n",
        "    print ('Epoch {}/{}, Loss: {:.4f}, Accuracy: {:.4f}'.format(epoch + 1, num_epochs, epoch_loss, epoch_acc*100))\n",
        "\n",
        "  return train_losses, train_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFYI98OtNjCc"
      },
      "source": [
        "def test(model, test_loader):\n",
        "  # Eval mode\n",
        "  model.eval()\n",
        "  test_acc = 0\n",
        "  correct = 0\n",
        "  for i, (images, labels) in enumerate(test_loader):\n",
        "    # Deactivate autograd engine (don't compute grads since we're not training)\n",
        "    with torch.no_grad():\n",
        "      images, labels = images.to(device), labels.to(device)\n",
        "      output = model(images)\n",
        "\n",
        "      # Calculate number of correct predictions\n",
        "      _, preds = torch.max(output, dim=1)\n",
        "      correct += (preds == labels).sum()\n",
        "\n",
        "  test_acc = correct / len(test_loader.dataset)\n",
        "  print('Test Accuracy: {:.4f}'.format(test_acc*100))\n",
        "\n",
        "  # Plot the images with predicted labels\n",
        "  plot_data(images.data.cpu().numpy(), preds.data.cpu().numpy(), test_loader.dataset.classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6pYjXgchc3E"
      },
      "source": [
        "Start training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ax9XMoe5H_ik"
      },
      "source": [
        "num_epochs = 10  # iterations\n",
        "train_losses, train_acc = train(num_epochs, model, train_loader, loss_func, optimizer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6ltVbRI4dr6"
      },
      "source": [
        "Plot training plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPJHC1_tIFAr"
      },
      "source": [
        "fig = plt.figure(figsize=(10,4))\n",
        "ax = fig.add_subplot(1,2, 1)\n",
        "ax.plot(np.arange(1,len(train_losses)+1),train_losses)\n",
        "plt.xlabel('Training loss')\n",
        "plt.ylabel('Epochs')\n",
        "ax.set_title('Loss vs Epochs')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYW-_wTfWvYl"
      },
      "source": [
        "#### Evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rReuhwjpXr5K"
      },
      "source": [
        "# Evaluate the model on testing data and plot predictions\n",
        "test(model, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puMO_nSBmUFq"
      },
      "source": [
        "### Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfozvSgV_mt5"
      },
      "source": [
        "Q 1: What is the ratio of parameters in single 5 x 5 kernel and equivalent stacked 3 x 3 kernels? Consider number of channels in input and output channels as C."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7LXbTpZnbT0"
      },
      "source": [
        "Q 2: How can you replace 7 x 7 convolution kernel using only 3 x 3 kernels? What would be ratio of parameters in this case? Consider number of channels in input and output channels as C."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. CNN Visualization"
      ],
      "metadata": {
        "id": "Tq_SyTxUbteW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ga0Az1zLcPVz"
      },
      "source": [
        "Save the conv layers and their weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "En_ZpAdpvVts"
      },
      "source": [
        "model_weights = [] # we will save the conv layer weights in this list\n",
        "conv_layers = [] # we will save the conv layers in this list\n",
        "# get all the model children as list\n",
        "model_children = list(model.children())\n",
        "\n",
        "# counter to keep count of the conv layers\n",
        "counter = 0\n",
        "# append all the conv layers and their respective weights to the list\n",
        "for i in range(len(model_children)):\n",
        "    if type(model_children[i]) == nn.Conv2d:\n",
        "        counter += 1\n",
        "        model_weights.append(model_children[i].weight)\n",
        "        conv_layers.append(model_children[i])\n",
        "    elif type(model_children[i]) == nn.Sequential:\n",
        "        for j in range(len(model_children[i])):\n",
        "            for child in model_children[i][j].children():\n",
        "                if type(child) == nn.Conv2d:\n",
        "                    counter += 1\n",
        "                    model_weights.append(child.weight)\n",
        "                    conv_layers.append(child)\n",
        "print(f\"Total convolutional layers: {counter}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N34IYC8Fziq1"
      },
      "source": [
        "# take a look at the conv layers and the respective weights\n",
        "for weight, conv in zip(model_weights, conv_layers):\n",
        "    # print(f\"WEIGHT: {weight} \\nSHAPE: {weight.shape}\")\n",
        "    print(f\"CONV: {conv} ====> SHAPE: {weight.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qhgNqhgcTpW"
      },
      "source": [
        "### Visualize the CONV layer filters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UCH5GBQpznZw"
      },
      "source": [
        "# Visualize the conv layer filters\n",
        "plt.figure(figsize=(20, 17))\n",
        "for i, filter in enumerate(model_weights[0]):\n",
        "    plt.subplot(8, 8, i+1) # (8, 8)\n",
        "    plt.imshow(filter[0, :, :].data.cpu().numpy(), cmap='gray')\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UPNns-hcbWK"
      },
      "source": [
        "### Visualize filter outputs on an image\n",
        "Get an image from test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rV_7rEx3zpuy"
      },
      "source": [
        "dataiter = iter(test_loader)\n",
        "for images, labels in dataiter:\n",
        "    img = images[1]\n",
        "    fig = plt.figure(figsize=(3, 3))\n",
        "    plt.imshow(img.reshape((28, 28)))\n",
        "    print(classes[labels[1].item()])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajfFCveAxpf-"
      },
      "source": [
        "Forward pass the image through saved conv layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VR6LxW9t0ADw"
      },
      "source": [
        "results = [conv_layers[0](img.to(device))]\n",
        "for i in range(1, len(conv_layers)):\n",
        "    # pass the result from the last layer to the next layer\n",
        "    results.append(conv_layers[i](results[-1]))\n",
        "# make a copy of the `results`\n",
        "outputs = results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkeMeBQdxwpP"
      },
      "source": [
        "Visualize features from each layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDuxp0nq0BEX"
      },
      "source": [
        "for num_layer in range(len(outputs)):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    layer_viz = outputs[num_layer][:, :, :]\n",
        "    layer_viz = layer_viz.data\n",
        "    print('Layer output size:', layer_viz.size())\n",
        "    for i, filter in enumerate(layer_viz):\n",
        "        plt.subplot(8, 8, i + 1)\n",
        "        plt.imshow(filter.cpu().numpy(), cmap='gray')\n",
        "        plt.axis(\"off\")\n",
        "    print(f\"Layer {num_layer} feature maps...\")\n",
        "    plt.show()\n",
        "    plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oniT6gixQZ7_"
      },
      "source": [
        "### Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5XatV34Qia0"
      },
      "source": [
        "Q: List a few practical applications of convolutional autoencoders.\n",
        "\n",
        "Image Denoising: They can be used to remove noise from images, making them clearer and easier to interpret.\n",
        "\n",
        "Image Compression: They can reduce the size of images without losing significant details, which is useful for storage and transmission.\n",
        "\n",
        "Anomaly Detection: They can identify unusual patterns in data, which is valuable in fields like fraud detection and predictive maintenance.\n",
        "\n",
        "Image Reconstruction: They can reconstruct images from corrupted or partially observed data, which can be useful in medical imaging and other fields.\n",
        "\n",
        "Super-Resolution: They can enhance the resolution of images, making low-resolution images appear sharper and more detailed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqMzkKLBq18x"
      },
      "source": [
        "Q: What change do we need to make for the autoencoder to reduce into PCA?\n",
        "\n",
        "Linear Activation Functions: Use linear activation functions (e.g., no activation function or identity) in both the encoder and decoder layers. This ensures that the transformations applied are linear, which is a key characteristic of PCA.\n",
        "\n",
        "Single Hidden Layer: Ensure the autoencoder has a single hidden layer. PCA can be thought of as a single-layer linear autoencoder.\n",
        "\n",
        "Mean Squared Error Loss: Use the mean squared error (MSE) as the loss function, which is common for both PCA and autoencoders.\n",
        "\n",
        "Zero-Centered Data: Ensure that the input data is zero-centered (i.e., the mean of each feature is zero). PCA assumes that the data is centered, so you should standardize your data accordingly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HjQRq6bVNtG"
      },
      "source": [
        "## 1. Effect of padding, kernel size and stride\n",
        "We will directly use convolution layer in the **Pytorch** framework. Refer [nn.Conv2d](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html) for more information about additional parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RKg8qtjP6af"
      },
      "source": [
        "# Import pytorch packages\n",
        "import torch\n",
        "from torch.nn import Conv2d\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -L -o 'lotus.jpg' 'https://drive.google.com/uc?export=download&id=1gQSQlrUws22KLRUacXwvN1G8FtIyhfGt'"
      ],
      "metadata": {
        "id": "daLFmh6HzGIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBcuce2bF0su"
      },
      "source": [
        "##### Convolution in pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRmaXjhVCrdJ"
      },
      "source": [
        "We will define a helper function to create an square vertical edge filter of given size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX6K7F6-dHhf"
      },
      "source": [
        "def generate_filter(k=3):\n",
        "  kernel = np.ones((k, k))\n",
        "  mid_index = k // 2\n",
        "  kernel[:, mid_index].fill(0)\n",
        "  kernel[:, mid_index+1:] *= -1\n",
        "  return kernel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X25gwS1GgMGY"
      },
      "source": [
        "We will create a helper function that takes one of the kernel elements, create a Convolution layer using pytorch and return the output image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-a6_FwTsgZ49"
      },
      "source": [
        "def apply_conv(image, kernel_size, padding=0, stride=1):\n",
        "\n",
        "  #--------IMAGE PREPROCESSING-------\n",
        "  # Convert image to tensor from numpy\n",
        "  image = torch.from_numpy(image)\n",
        "  # Pytorch requires input to convolution in (N,C,H,W), where N = batch size and C=#channels in input\n",
        "  input = image.view((1,1,image.shape[0], image.shape[1]))\n",
        "\n",
        "  # --------------KERNEL-------------\n",
        "  # Create a nxn kernel\n",
        "  kernel = generate_filter(kernel_size)\n",
        "\n",
        "  # Create a tensor from the numpy array\n",
        "  kernel = torch.from_numpy(kernel.astype(np.float32))\n",
        "\n",
        "  # Pytorch requires kernel of shape (N,C,H,W), where N = batch size and C=#channels in input\n",
        "  kernel = kernel.view((1,1,kernel.shape[0], kernel.shape[1]))\n",
        "\n",
        "  # ---------CONVOLUTION LAYER--------\n",
        "  #1 input image channel, 1 output channels, nxn square convolution with padding on all 4 sides\n",
        "  conv = Conv2d(in_channels=1, out_channels=1, kernel_size=kernel.shape, padding=padding, stride=stride)\n",
        "\n",
        "  # Set the kernel weights in the convolution layer\n",
        "  conv.weight = torch.nn.Parameter(kernel)\n",
        "\n",
        "  # ---------APPLY CONVOLUTION--------\n",
        "  output = conv(input / 255.)  # Getting input from 0 to 1\n",
        "  output_img = output.data.numpy()  # Tensor to back in numpy\n",
        "  output_img = output_img.reshape((-1, output_img.shape[-1])) # Reshape to 2D image\n",
        "\n",
        "  return output_img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6-_WvVRdp6l"
      },
      "source": [
        "##### Effect of Padding\n",
        "Change the padding value with the slider. What do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ehT5KaDJeESD"
      },
      "source": [
        "#@title Effect of padding { run: \"auto\", vertical-output: true, display-mode: \"both\" }\n",
        "\n",
        "# Note:After running this cell manually, it will auto-run if you\n",
        "# change the selected value.\n",
        "\n",
        "# Our original lotus image\n",
        "image = cv2.imread('lotus.jpg', 0)\n",
        "\n",
        "\n",
        "# Apply 3x3 convolution to image with given padding 1 on all 4 sides\n",
        "padding = 2 #@param {type:\"slider\", min:1, max:20, step:1}\n",
        "n = apply_conv(image, 3, padding=padding)\n",
        "\n",
        "# Plot the results\n",
        "plt.imshow(n, cmap='gray')\n",
        "plt.title('Padding={}\\nShape: {}'.format(padding, str(n.shape)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyFlTUw7exTI"
      },
      "source": [
        "As you observed, the output shape changes with padding. More the padding, bigger will be the image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVThLy3OfBi4"
      },
      "source": [
        "##### Effect of Kernel size\n",
        "Change the kernel size with the slider. What do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYoc2xwTlBj_"
      },
      "source": [
        "#@title Effect of Kernel size { run: \"auto\", vertical-output: true, display-mode: \"both\" }\n",
        "# Our original lotus image\n",
        "image = cv2.imread('lotus.jpg', 0)\n",
        "\n",
        "# Apply 3x3 convolution to image\n",
        "K = 19 #@param {type:\"slider\", min:3, max:21, step:2}\n",
        "n = apply_conv(image, K)\n",
        "\n",
        "# Plot result\n",
        "plt.imshow(n, cmap='gray')\n",
        "plt.title('K = {}\\nShape = {}'.format(K, str(n.shape)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2oKNN4gBnzOc"
      },
      "source": [
        "Thus, we conclude that output image becomes blurry with increase in kernel size as summation occurs over larger neighbourhood. Smaller kernel size is used to capture details whereas larger kernel captures bigger elements in image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DJoZNQOmlT3"
      },
      "source": [
        "##### Effect of Stride\n",
        "Change the stride value with the slider. What do you observe?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zISU25SBj1O7"
      },
      "source": [
        "#@title Effect of Stride { run: \"auto\", vertical-output: true, display-mode: \"both\" }\n",
        "# Our original lotus image\n",
        "image = cv2.imread('lotus.jpg', 0)\n",
        "\n",
        "# Apply 3x3 convolution to image\n",
        "stride = 9 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "n = apply_conv(image, 3, stride=stride)\n",
        "\n",
        "# Plot result\n",
        "plt.imshow(n, cmap='gray')\n",
        "plt.title('Stride = {}\\nShape = {}'.format(stride, str(n.shape)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions\n",
        "\n",
        "1. Does increasing stride increase output image size?\n",
        "\n",
        "Answer) When you increase the stride in a convolutional layer, it actually decreases the size of the output image. A larger stride means the filter skips more pixels, resulting in a smaller output.\n",
        "\n",
        "2. Does increasing padding increase output image size?\n",
        "\n",
        "Answer) Increasing padding increases the output image size. Padding adds extra pixels around the input image, resulting in a larger output."
      ],
      "metadata": {
        "id": "SQcT-SyR8LPS"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWI03dYYR2xf"
      },
      "source": [
        "## 2. Pooling\n",
        "Strides, actually downsample the image but a more robust and common approach is pooling. It may be useful when we do not require finer details but important structural elements. Here, we will see an example of max pooling and average pooling on a simple 2D image matrix. Refer [nn.MaxPool2d](https://pytorch.org/docs/stable/generated/torch.nn.MaxPool2d.html) and [nn.AvgPool2d](https://pytorch.org/docs/stable/generated/torch.nn.AvgPool2d.html) for the documentation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVOCZ80mnrdf"
      },
      "source": [
        "Max Pooling & Average Pooling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lD1mlfc8JOjs"
      },
      "source": [
        "from torch.nn import MaxPool2d, AvgPool2d\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "\n",
        "def plot_images(images, titles, tick_params=True):\n",
        "  n = len(images)\n",
        "  fig = plt.figure(figsize=(10,4))\n",
        "  for i in range(n):\n",
        "    ax = fig.add_subplot(1,n,i+1)\n",
        "    if len(images[i].shape) == 2:\n",
        "      ax.imshow(images[i], cmap='gray',\n",
        "                extent=(0,images[i].shape[1], images[i].shape[0], 0))\n",
        "    else:\n",
        "      ax.imshow(images[i])\n",
        "    ax.set_title(titles[i])\n",
        "    if not tick_params:\n",
        "      plt.tick_params(axis='both', labelbottom=False, bottom=False,\n",
        "                labelleft=False, left=False)\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "# 2D image\n",
        "image = np.array([\n",
        "   \t\t[2, 0, 0, 1, 6, 0, 5, 3],\n",
        "\t\t[0, 6, 5, 9, 1, 5, 8, 0],\n",
        "\t\t[0, 6, 0, 1, 2, 3, 4, 4],\n",
        "\t\t[7, 0, 3, 1, 4, 2, 0, 1],\n",
        "\t\t[3, 7, 0, 5, 6, 0, 5, 0],\n",
        "\t\t[0, 4, 6, 1, 1, 0, 0, 1],\n",
        "\t\t[3, 5, 0, 2, 2, 0, 9, 0],\n",
        "\t\t[1, 0, 3, 1, 7, 0, 0, 0]])\n",
        "\n",
        "# Saving output for plots\n",
        "output = []\n",
        "titles = []\n",
        "\n",
        "output.append(image)\n",
        "titles.append('Image')\n",
        "\n",
        "image = torch.from_numpy(image.astype(np.float32))\n",
        "input = image.view((1,1,image.shape[0], image.shape[1]))\n",
        "\n",
        "#----------MAX POOLING LAYER--------\n",
        "pool_layer = MaxPool2d(kernel_size=4, stride=4)\n",
        "op = pool_layer(input)\n",
        "max_output_img = op.data.numpy()  # Tensor to back in numpy\n",
        "max_output_img = max_output_img.reshape((-1, max_output_img.shape[-1]))\n",
        "print('Max Pooling:\\n', max_output_img)\n",
        "print()\n",
        "output.append(max_output_img)\n",
        "titles.append('Max Pool output')\n",
        "\n",
        "#----------AVERAGE POOLING LAYER--------\n",
        "pool_layer = AvgPool2d(kernel_size=4, stride=4)\n",
        "op = pool_layer(input)\n",
        "avg_output_img = op.data.numpy()  # Tensor to back in numpy\n",
        "avg_output_img = avg_output_img.reshape((-1, avg_output_img.shape[-1]))\n",
        "print('Avg Pooling:\\n',avg_output_img)\n",
        "print()\n",
        "\n",
        "output.append(avg_output_img)\n",
        "titles.append('Avg Pool output')\n",
        "\n",
        "plot_images(output, titles, tick_params=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Questions\n",
        "\n",
        "1. Can you think of any other pooling other than max and avg?\n",
        "\n",
        "Answer) Here are a few other pooling methods:\n",
        "\n",
        "Global Pooling: Takes the maximum or average across the entire feature map.\n",
        "\n",
        "L2 Pooling: Uses the L2 norm over the pooling window.\n",
        "\n",
        "Stochastic Pooling: Randomly selects a value based on a probability distribution.\n",
        "\n",
        "Spatial Pyramid Pooling: Pools features at multiple spatial scales.\n",
        "\n",
        "Adaptive Pooling: Produces a fixed-size feature map regardless of input size."
      ],
      "metadata": {
        "id": "cp9yq6LM8mBF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQqIVNVYnA90"
      },
      "source": [
        "## 3. Fine-tuning and transfer learning\n",
        "\n",
        "Now , we will perform image classification using pretrained CNN models (transfer learning). We will understand two approaches, Fine-tuning and Feature extraction using ResNet architecture to train a model to perform traffic sign classification.\n",
        "\n",
        "To make your task easier, we provide you the starter code to perform the lab exercises. It is expected that you should try to understand what the code does and analyze the output. We will be using Pytorch framework for the implementation of this lab. The training hyperparameters that are used in the code may not be the best to minimize training time according to lab scope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7KBF9XrGaSy"
      },
      "source": [
        "### German Traffic Sign classification\n",
        "When a task involves training a CNN on a dataset of images, our first instinct would be to train the network from scratch. However, in practice, CNN has a huge number of parameters, often in the range of millions. Training a CNN on a small dataset greatly affects the network's ability to generalize, often resulting in overfitting.\n",
        "Therefore, in practice, one would fine-tune existing networks that are trained on a large dataset like the ImageNet (1.2M labeled images) by continue training it (i.e. running back-propagation) on the smaller dataset we have. Provided that our dataset is not drastically different in context to the original dataset (e.g. ImageNet), the pre-trained model will already have learned features that are relevant to our own classification problem.  Here, we will understand the Fine-tuning and Feature extraction approach to transfer learning. In the first one, we will take a pretrained ResNet model and replace the classifier to train it on our dataset. In the second approach, we will freeze the weights of the entire network except the classifier and train it on our data. We will thus, analyse the model performance in both cases. The German Traffic Sign Recognition Benchmark (GTSRB) dataset contains 43 classes of traffic signs, with varying light conditions and rich backgrounds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ins6ETpApPJA"
      },
      "source": [
        "# Import packages\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import time\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHCZ__V1uK5R"
      },
      "source": [
        "# Device configuration (whether to run on GPU or CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4748CHsYoI2"
      },
      "source": [
        "# Set seeds for reproducibility\n",
        "seed = 0\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "up9pTZ_fuROx"
      },
      "source": [
        "### Load German Traffic Sign dataset\n",
        "To get an idea of using our own datasets with Pytorch, this time, we will not use Pytorch's builtin datasets. The dataset we will use has more than 50K samples. To make the scenario more realistic, the number of samples in each class is limited to 200 only. And we have also reduced the test set."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown --id 1V7dt70fz_AKRJlttyjnrtFpuJDLXr15x"
      ],
      "metadata": {
        "id": "dHm9cbhE2I5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d13S9bwuiQy",
        "outputId": "7a85be66-30ad-440a-af0c-6969c0dbcc18",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Unzip\n",
        "!unzip -q german_traffic_signs_dataset.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "replace german_traffic_signs_dataset/Test/0/00243.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HQXR2nRP3HJ"
      },
      "source": [
        "The dataset is stored in a folder structure where samples are separated in classwise folders. We can load the entire dataset using Pytorch's ['ImageFolder'](https://pytorch.org/vision/stable/datasets.html#ImageFolder) class. Then, we can see it like any built-in dataset. As the images are of varying shape, we will resize them to fixed dimensions (224,224) and normalize them in range [0,1]. We will here use data augmentation techniques like Gaussian blur and affine transformation to augment the data. This will increase variations in our data and help our model to generalize well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6_183iPGunN4"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.GaussianBlur(3),\n",
        "            transforms.RandomAffine(0, translate=(0.3,0.3), shear=5),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = ImageFolder('german_traffic_signs_dataset/Train', transform=transform)\n",
        "testset = ImageFolder('german_traffic_signs_dataset/Test', transform=transform)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1yRmcSCu4Yu"
      },
      "source": [
        "#### Train, validation and test dataloaders\n",
        "We will split the trainset further to create train-validation split. We will only train on train data and evaluate the model on validation data at each step. The validation metrics helps us to understand whether model is overfitting the data or not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GomEiTYu1rI"
      },
      "source": [
        "# Shuffle and split train set into 80% training and 20% validation set\n",
        "val_split = 0.2\n",
        "indices = np.arange(len(trainset))\n",
        "np.random.shuffle(indices)\n",
        "partition = int((1-val_split)*len(trainset))\n",
        "\n",
        "#SubsetRandomSampler will only sample examples from the given subset of data\n",
        "train_loader = DataLoader(trainset, shuffle=False, sampler=SubsetRandomSampler(indices[:partition]), batch_size=64, num_workers=2)\n",
        "val_loader = DataLoader(trainset, shuffle=False, sampler=SubsetRandomSampler(indices[partition:]), batch_size=64, num_workers=2)\n",
        "\n",
        "dataloaders = {'train': train_loader, 'val': val_loader}\n",
        "dataset_sizes = {'train': partition, 'val': len(train_loader.dataset) - partition}\n",
        "\n",
        "test_loader = DataLoader(testset, shuffle=False, batch_size=64, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxDtEgIZvI6r"
      },
      "source": [
        "# Print dataset information\n",
        "print('Number of training images: ', dataset_sizes['train'])\n",
        "print('Number of validation images: ', dataset_sizes['val'])\n",
        "print('Number of test images: ', len(test_loader.dataset))\n",
        "print('Number of classes: ', len(trainset.classes))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8xz7oSyvUCu"
      },
      "source": [
        "Helper functions for training/testing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to show an image\n",
        "def plot_image(img):\n",
        "    img = img / 2 + 0.5                         # unnormalize the image\n",
        "    npimg = img.numpy()                         # torch to numpy\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))  # as torch image is (C, H, W)\n",
        "    plt.show()\n",
        "\n",
        "# Get some random training images from dataloader\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Plot images\n",
        "plot_image(torchvision.utils.make_grid(images[:20], nrow=5))\n"
      ],
      "metadata": {
        "id": "psftUClpcuCQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJKu5vTyaF-C"
      },
      "source": [
        "def train_model(model, criterion, optimizer, dataloaders, num_epochs=10):\n",
        "    since = time.time()\n",
        "\n",
        "    # best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    train_losses = []\n",
        "    train_acc = []\n",
        "    val_losses = []\n",
        "    val_acc = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # Forward\n",
        "                # Enable grads if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # Backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # Running loss and correct predictions\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            # Save loss and acc values\n",
        "            if phase == 'train':\n",
        "              train_losses.append(epoch_loss)\n",
        "              train_acc.append(epoch_acc)\n",
        "            else:\n",
        "              val_losses.append(epoch_loss)\n",
        "              val_acc.append(epoch_acc)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc*100))\n",
        "\n",
        "            # Save the best validation accuracy\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc*100))\n",
        "\n",
        "    return train_losses, val_losses, train_acc, val_acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZWTjtDwb8L_"
      },
      "source": [
        "def test_model(model, test_loader):\n",
        "    model.eval()\n",
        "    test_acc = 0\n",
        "    correct = 0\n",
        "    for i, (images, labels) in enumerate(test_loader):\n",
        "      with torch.no_grad():\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        output = model(images)\n",
        "        _, preds = torch.max(output, dim=1)\n",
        "        correct += (preds == labels).sum()\n",
        "\n",
        "    test_acc = correct / len(test_loader.dataset)\n",
        "    print('Test Accuracy: {:.4f}'.format(test_acc*100))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QExzv8covZIR"
      },
      "source": [
        "### 1. Finetuning\n",
        "Here, we will load a pretrained model ResNet18 available in Pytorch and reset final fully connected layer. The model is trained on ImageNet dataset which is a large dataset containing 1000 classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hG2XitCvX4B"
      },
      "source": [
        "# Load pretrained model\n",
        "model = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "# Reset classifier to 43 output units (number of classes in our dataset)\n",
        "model.fc = nn.Linear(model.fc.in_features, 43)\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yC999ilMTx52"
      },
      "source": [
        "#### Define loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2Sey-3y3UKJ"
      },
      "source": [
        "# Cross Entropy loss for multi-class classification\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7YjI7Y0T1f2"
      },
      "source": [
        "#### Define optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNjCtOyBT419"
      },
      "source": [
        "# SGD optimizer with momentum\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EgpDdgB0wGAU"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaC-PUeNv8xJ"
      },
      "source": [
        "# Accuracy on test data before training\n",
        "test_model(model, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7p5M87hTv9E7"
      },
      "source": [
        "history = train_model(model, criterion, optimizer, dataloaders, num_epochs=6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS8lmAzgpPJE"
      },
      "source": [
        "Plot training plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iERYvlwpPJE"
      },
      "source": [
        "fig = plt.figure(figsize=(10,4))\n",
        "ax = fig.add_subplot(1,2, 1)\n",
        "ax.plot(np.arange(1,len(history[0])+1),history[0])\n",
        "ax.plot(np.arange(1,len(history[1])+1),history[1])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train Loss', 'Val Loss'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhVyadHIU9FY"
      },
      "source": [
        "#### Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_4-zXsxVB3i"
      },
      "source": [
        "# Accuracy on test data after training\n",
        "test_model(model, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Id93lgE1kdd"
      },
      "source": [
        "### 2. Feature Extraction\n",
        "Here, in the second approach, we will create a new instance of network and freeze entire network parameters except the final layer. We need to set ***requires_grad == False*** to freeze the parameters so that the gradients are not computed in backward()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfrJQuaVv9Hs"
      },
      "source": [
        "# Load pretrained model\n",
        "model_conv = torchvision.models.resnet18(pretrained=True)\n",
        "\n",
        "# Freeze all parameters\n",
        "for param in model_conv.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Parameters of new classifier have requires_grad=True by default\n",
        "# so grads will be computed for classifier only\n",
        "num_ftrs = model_conv.fc.in_features\n",
        "model_conv.fc = nn.Linear(num_ftrs, 43)\n",
        "\n",
        "model_conv = model_conv.to(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that only parameters of final layer are being optimized as opposed to before.\n",
        "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.01, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cgk1ahfE17Rp"
      },
      "source": [
        "# Evaluate model on test data before training\n",
        "print('Before training')\n",
        "test_model(model_conv, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNC6rLI32G65"
      },
      "source": [
        "####Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hLRYy2rv9OS"
      },
      "source": [
        "history = train_model(model_conv, criterion, optimizer_conv, dataloaders, num_epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HuSbIAfI2nnc"
      },
      "source": [
        "fig = plt.figure(figsize=(10,4))\n",
        "ax = fig.add_subplot(1,2, 1)\n",
        "ax.plot(np.arange(1,len(history[0])+1),history[0])\n",
        "ax.plot(np.arange(1,len(history[1])+1),history[1])\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend(['Train Loss', 'Val Loss'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAvn3h1g2sR-"
      },
      "source": [
        "#### Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ylntBgw2sR_"
      },
      "source": [
        "# Accuracy on test data after training\n",
        "test_model(model_conv, test_loader)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45jHKIHt6iM-"
      },
      "source": [
        "### Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mjppKyXWKaR"
      },
      "source": [
        "Q 1: Why do you think the network did not achieve good test accuracy in the feature extraction approach?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Insufficient Training Data: Not enough labeled data.\n",
        "\n",
        "Overfitting: Good on training data, bad on test data.\n",
        "\n",
        "Hyperparameters: Poor choice of learning rate, batch size, etc.\n",
        "\n",
        "Feature Selection: Irrelevant features.\n",
        "\n",
        "Data Quality: Noisy or inconsistent data.\n",
        "\n",
        "Model Architecture: Unsuitable design."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-FfAbYBW_qA"
      },
      "source": [
        "Q 2: Can you think of a scenario where the feature extraction approach would be preferred compared to fine tuning approach?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Limited Computational Resources: Feature extraction requires less computational power and memory compared to fine-tuning the entire network.\n",
        "\n",
        "Small Dataset: When you have a limited amount of labeled data, extracting features from a pre-trained model and training only the classifier can prevent overfitting.\n",
        "\n",
        "Transfer Learning Across Tasks: When the pre-trained model was trained on a dataset similar to your target dataset, the features extracted can be highly relevant and useful.\n",
        "\n",
        "Rapid Prototyping: If you need to quickly test and validate different models, feature extraction allows for faster experimentation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOHxNtgt3qM2"
      },
      "source": [
        "Q 4: Which other data augmentations can we used to augment the data?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Rotation: Rotate images by a certain angle.\n",
        "\n",
        "Translation: Shift images horizontally or vertically.\n",
        "\n",
        "Scaling: Resize images by scaling them up or down.\n",
        "\n",
        "Shearing: Apply a shear transformation to tilt the image.\n",
        "\n",
        "Flipping: Flip images horizontally or vertically.\n",
        "\n",
        "Cropping: Randomly crop sections of the image.\n",
        "\n",
        "Color Jitter: Randomly change the brightness, contrast, saturation, and hue.\n",
        "\n",
        "Gaussian Noise: Add random noise to the images.\n",
        "\n",
        "Cutout/Random Erasing: Randomly mask out sections of the image.\n",
        "\n",
        "Perspective Transform: Apply a random perspective transformation."
      ]
    }
  ]
}